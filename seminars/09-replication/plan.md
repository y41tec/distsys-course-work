# Семинар 9: Репликация

Почитать заранее:
- https://youtu.be/gkg-FAEXIkY
- https://en.wikipedia.org/wiki/CAP_theorem
- https://jvns.ca/blog/2016/11/19/a-critique-of-the-cap-theorem/ 
- RAID 0, 1, 4, 5 https://en.wikipedia.org/wiki/Standard_RAID_levels
- [опционально] https://igoro.com/archive/how-raid-6-dual-parity-calculation-works/

## Репликация в нашем игрушечном Key Value

Как решать задачу, когда я хочу иметь KV, который переживает падение одной любой ноды на время?

### Простейший алгоритм

В простейшем алгоритме репликации, реализованном в [../08-balancing-sharding/kv](./08-balancing-sharding/kv/readme.md) с `mode: replication`, мы пишем значение на все ноды (best effort), а читаем первый успешный ответ (`200 OK`). У этого способа есть очевидная проблема — может так получиться, что состояние нод в некоторый момент окажется разным и само не восстановится, а иметь разные значения для ключа на разных узлах это самое плохое, что мы можем получить в реплицированном key-value.

### Кворумные запись/чтение

Для переключения прокси в режим кворумных чтений и записи (без версий), надо указать `mode: replication-quorum`. В этом режиме мы на ноды и если было `> N/2` успехов, то возвращаем `200` на запрос записи. Аналогично с чтением — мы считаем значения и возвращаем только если у какого-то значения победило большинство.

### Совершенствование репликации

Sloppy quorum & hinted handoff (параграф 4, особенно 4.6-4.9): https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf

### Оверхед на хранение

Для того, чтобы переживать `< N/2` отказов с сохранением гарантий, нам приходится дублировать данные `N` раз. Это дорого.

## RAID и Erasure

Существует гибридный подход к хранению данных на дисках (изначально появился для медленных и маленьких HDD в 90х), но на самом деле его можно использовать и в распределенных хранилищах. Базово идея звучит очень просто — если мы хотим в системе из `N` шардов данных переживать падение одной ноды, то нам нужен способ восстановить потерянные данные. Базовая идея заключается в _parity_ блоке — `X` байт данных делятся на равные `N` кусков по `X/N` байт, а ещё один кусок является результатом `xor` всех кусков. Его размер тоже будет `X / N` байт. В силу обратимости операции, мы можем восстановить любой потерянный кусок данных, если у нас есть все остальные `N-1`. В [../08-balancing-sharding/kv](./08-balancing-sharding/kv/readme.md) c `mode: replication-raid3` реализован подобный механизм. Чтобы увеличить количество кусков, которые можно восстановить, прибегают к более сложным операциям, например тут можно почитать про [RAID 6](https://igoro.com/archive/how-raid-6-dual-parity-calculation-works/). Самым распространенным в продакшене для больших хранилищ является ещё более сложный способ: [LRC от Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/LRC12-cheng20webpage.pdf).
