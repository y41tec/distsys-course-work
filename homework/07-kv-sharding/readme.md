# Распределенное key-value хранилище с шардингом

В этой задаче вам надо реализовать распределенную систему, в которой можно хранить записи вида _(ключ, значение)_. Ключами и значениями могут быть произвольные строки. В системе может храниться только одна запись с данным ключом. Для каких-то ключей записи могут отсутствовать. Распределение используемых ключей может быть любым, необязательно равномерным. Система должна поддерживать стандартные операции для работы с данными в подобных key-value хранилищах: записать значение для ключа, прочитать значение по ключу и удалить ключ.

Система состоит из нескольких идентичных узлов, на каждом из которых запущен процесс _StorageNode_. Для поддержки горизонтального масштабирования каждый узел отвечает за хранение только некоторой части данных. А именно, для любого возможного значения ключа должен быть определен (один и только один) узел, "отвечающий" за хранение соответствующей записи. Таким образом, каждый узел в системе отвечает за некоторое подмножество ключей и хранение связанных с этими ключами записей. Хранить данные на диске не требуется, достаточно держать все записи в памяти.

Нагрузка на узлы системы должна распределяться равномерно. А именно, каждый узел должен хранить примерно одинаковую долю записей - в идеале _R/N_ записей, где _R_ - суммарное число хранимых записей, а _N_ - число узлов. В тестах допустимым считается отклонение не более чем на 10% от идеального значения.

В ходе работы системы в неё могут добавляться новые узлы, а существующие - удаляться. Добавление и удаление узлов происходит по команде администратора. При этом каждому узлу в системе отправляется соответствующее уведомление. Как обычно, узлы также могут внезапно отказывать. Чтобы не создавать зависимость от предыдущего ДЗ, будем считать, что отказы узлов уже обнаруживаются и обрабатываются каким-то образом. Соответственно, от вас не требуется реализовывать функционал group membership. В случае отказа узла и невозможности быстро восстановить его работу, этот узел может быть удален из системы через описанный выше механизм. Хранимые на отказавшем узле записи становятся временно недоступны или теряются насовсем. Эту проблему мы будем решать позже в рамках следующего задания.

При изменении состава узлов должна происходить _перебалансировка_ - перераспределение ключей (зон ответственности) и записей (хранимых данных) между узлами для восстановления ранее описанных свойств. Во-первых, для любого возможного значения ключа должен существовать узел, отвечающий за него. Во-вторых, каждый узел по-прежнему должен хранить примерно одинаковую долю записей. Таким образом, при добавлении нового узла, он должен принять на себя некоторую часть ключей и хранимых записей. А при отключении узла, ключи, за которые он отвечал, и хранимые им записи должны распределяться между оставшимися узлами. Перебалансировка сопряжена с расходами на перемещение хранимых данных между узлами. Требуется минимизировать эти расходы - в идеале должно перемещаться только _R/N_ записей. В тестах допустимым считается отклонение не более чем на 10% от идеального значения. Для простоты предполагается, что во время перебалансировки запросы на чтение или изменение ключей не поступают (например, могут буферизоваться где-то до окончания процесса).

Клиенты системы не знают о распределении данных между узлами и могут отправлять свои запросы к любому узлу. Соответственно узел должен уметь обслуживать запросы на чтение и запись к _любым_ ключам. Если узел не отвечает за данный ключ, то он должен переслать запрос правильному узлу, дождаться ответа и вернуть его локальному клиенту. Таймауты и ошибки в ответах поддерживать необязательно. По умолчанию, вы можете поддерживать на каждом узле некоторую структуру данных, с помощью которой можно локально определить узел отвечающий за ключ (т.е. реализовать zero-hop DHT). Однако размер этой структуры не должен зависеть от числа хранимых в системе записей, то есть нельзя просто хранить пары ключ-узел, что будет очень неэффективно.

Чтобы набрать 10 баллов, дополнительно к базовым требованиям надо поддержать любое из следующих:

- вычисление узла, отвечающего за заданный ключ, выполняется не хуже, чем за _O(log N)_, где _N_ - число узлов (+ убедительное обоснование в отчёте);
- каждый узел хранит только часть информации о других узлах (размером менее чем _O(N)_) и находит узел, отвечающий за ключ, за _O(log N)_ шагов (т.е. реализована полноценная DHT типа Chord).

Если вам плохо понятны некоторые требования, изучите соответствующие тесты - это часть условия задачи.

## Реализация

Для реализации и тестирования решения используется учебный фреймворк dslab-mp (см. материалы первого семинара). В папке задачи размещена заготовка для решения [solution.py](solution.py). Вам надо доработать реализацию класса `StorageNode` так, чтобы проходили все тесты.

При инициализации узлу передается его уникальный id, а также список id всех узлов в системе на данный момент. В начале теста узлы инициализируются одновременно, то есть каждый из них получает полный список узлов на момент старта системы.

Узел должен поддерживать обработку следующих локальных сообщений (форматы запросов и ответов описаны в заготовке):
- _GET(key)_ - вернуть значение записи с ключом `key` (может выдать пустое значение, если записи с этим ключом нет),
- _PUT(key, value)_ - сохранить запись с ключом `key` и значением `value`,
- _DELETE(key)_ - удалить запись с ключом `key`,
- _NODE_ADDED_ - уведомление о добавлении нового узла в систему,
- _NODE_REMOVED_ - уведомление об удалении узла из системы,
- _COUNT_RECORDS_ - возвращает число записей, хранимых на данном узле (используется для тестирования),
- _DUMP_KEYS_ - возвращает ключи записей, хранимых на данном узле (используется для тестирования).

Для взаимодействия между узлами вы можете использовать любые собственные типы сообщений.

### Рекомендации и замечания

1. Как вы могли догадаться, распределять по шардам необходимо только запросы _GET_, _PUT_ и _DELETE_. Кроме того, необходимо реализовать перебалансировку ключей при получении _NODE_ADDED_ или _NODE_REMOVED_.
2. Помните, что во время перебалансировки запросы на чтение или изменение ключей не поступают. Вашей реализации необязательно поддерживать узлы в консистентном состоянии в любой момент времени в процессе перебалансировки - достаточно, чтобы они пришли в такое состояние через какое-то ограниченное число сообщений.
3. Учтите, что в тестах окончание перебалансировки определяется по косвенным признакам, таким как отсутствие изменений числа хранимых записей на узлах (см. функцию `step_until_stabilized`), поскольку мы не стали требовать от вас возвращать явные ответы на _NODE_ADDED_ и _NODE_REMOVED_. 
4. Не требуется реализовывать подтверждения и ретраи передаваемых между узлами запросов, в тестах сеть надежная.

## Тестирование

Перед запуском тестов убедитесь, что на вашей машине [установлен Rust](https://www.rust-lang.org/tools/install). 

Тесты находятся в папке `test`. Для запуска тестов перейдите в эту папку и выполните команду: `cargo run --release`. Запустить только один из тестов можно с помощью опции `-t`. По умолчанию вывод тестов не содержит трассы (последовательности событий во время выполнения каждого из тестов), а только финальную сводку. Включить вывод трасс можно с помощью флага `-d`. Все доступные опции можно посмотреть с помощью `cargo run --release -- --help`.

В тестах MC NORMAL и MC NODE REMOVED вы можете столкнуться с ошибкой "state depth exceeds maximum allowed depth...". Эти тесты основаны на model checking (см. первый семинар) и, чтобы ограничить время тестирования, глубина проверяемых состояний системы в них ограничена. Соответственно, если за заданное число шагов обработка запроса не завершится, то будет выдана эта ошибка. Ограничения на глубину заданы так, чтобы в них легко укладывалась эффективное решение, во всяком случае для zero-hop DHT. В случае такой ошибки рекомендуется доработать решение, чтобы избавиться от избыточных действий. Если вы реализуете полноценную DHT на 10 баллов, и никак не укладываетесь в лимиты на глубину, вы можете повысить их в тестах, убедительно обосновав необходимость этого в отчёте.

Если вы найдете ошибки или требования из условий, которые не покрывают наши тесты, то вы можете получить за это бонусы. Для этого надо включить в отчёт описание ситуации, которую не ловят тесты, добавив при необходимости пример решения с ошибкой. За это полагается 1 балл. Если вы также реализуете тесты, которые ловят найденную проблему, или хотя бы опишите их логику, то получите еще 1 балл. Готовые тесты оформляйте как pull request в репозиторий курса.

## Оценивание

Компоненты задачи и их вклад в оценку:
- Отчёт с описанием вашего решения в файле `solution.md` - обязательно, без него проверка производиться не будет.
- Поддержка базовых операций с данными (тесты: SINGLE NODE, INSERTS, DELETES, MEMORY OVERHEAD, MC NORMAL) - 3 балла.
- Поддержка добавления и удаления узлов (тесты: NODE..., MC NODE REMOVED, MIGRATION, SCALE UP DOWN) - 3 балла. 
- Равномерное распределение данных и минимизация их перемещения при перебалансировке (тесты DISTRIBUTION...) - 2 балла.
- Одно из дополнительных требований (см. выше) - 2 балла.

## Сдача

Следуйте стандартному [порядку сдачи заданий](../readme.md).
